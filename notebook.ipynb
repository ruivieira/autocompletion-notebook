{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probabilistic classifier.\n",
    "Given $n$ _attributes_, that is\n",
    "\n",
    "$$\n",
    "\\text{attributes} = \\lbrace{a_1, a_2, a_3, \\dots, a_n \\rbrace},\n",
    "$$\n",
    "\n",
    "and a set of $k$ _labels_, we can calculate the conditional probability of each label given a certain set of attributes. That is:\n",
    "\n",
    "$$\n",
    "p\\left(L_k|a_1, a_2, a_3,\\dots,a_n\\right)\n",
    "$$.\n",
    "\n",
    "This is done by direct application of Baye's theorem:\n",
    "\n",
    "$$\n",
    "p\\left(L_k|a_1,a_2,a_3,\\dots,a_n\\right) = \\frac{p\\left(L_k\\right)p\\left(a_1,a_2,a_3,\\dots,a_n|L_k\\right)}{p\\left(a_1,a_2,a_3,\\dots,a_n\\right)}\n",
    "$$\n",
    "\n",
    "## Example\n",
    "\n",
    "For the following minimal example a Java implementation of the Naive Bayes algorithm will be used (available at https://github.com/ruivieira/java-naive-bayes).\n",
    "\n",
    "Consider a simple IT purchasing system where the user can choose a laptop brand (`Apple` or `Lenovo`) for use in a specific department (`design` or `accounting`) in one of two offices (`US` or `UK`).\n",
    "\n",
    "In this case, NB will try to classify the laptop brand according to the historical purchase data. It is clear then that the attributes will be:\n",
    "\n",
    "$$\n",
    "    a = \\lbrace\\text{user}, \\text{department}, \\text{office}\\rbrace\n",
    "$$\n",
    "\n",
    "and the labels will be\n",
    "\n",
    "$$\n",
    "    L = \\lbrace\\text{Brand A}, \\text{Brand B}\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.ruivieira:naivebayes:0.1-SNAPSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.ruivieira.ml.naivebayes.NaiveBayes;\n",
    "import org.ruivieira.ml.naivebayes.Model;\n",
    "\n",
    "import java.util.Map;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model model = Model.create();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by adding the first record. User Anna buys an Apple for the US design department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(new String[]{\"Anna\", \"design\", \"US\"}, \"Apple\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to predict that label (outcome) for any of the attributes, the result will be unsurprising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna: {Apple=100.0}\n",
      "design: {Apple=100.0}\n",
      "US: {Apple=100.0}\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Anna: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"US: \" + naiveBayes.classify(new String[]{\"US\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add a purchase for a Lenovo for US accounting department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna: {Lenovo=50.0, Apple=50.0}\n",
      "design: {Lenovo=5.0E-9, Apple=50.0}\n",
      "US: {Lenovo=50.0, Apple=50.0}\n"
     ]
    }
   ],
   "source": [
    "model.train(new String[]{\"Anna\", \"accounting\", \"US\"}, \"Lenovo\");\n",
    "naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Anna: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"US: \" + naiveBayes.classify(new String[]{\"US\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing we couldn't figure out ourselves, yet. Anna is 50/50 as likely to buy a Lenovo or an Apple. The design department is more likely to get an Apple (50% vs. ~0%) and the US office is as likely to get a Lenovo or an Apple.\n",
    "\n",
    "Let's now add a third user, Bill. Bill makes the same purchasing choices as Anna, but he also buys for the UK office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(new String[]{\"Bill\", \"accounting\", \"US\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Bill\", \"design\", \"US\"}, \"Apple\");\n",
    "model.train(new String[]{\"Bill\", \"accounting\", \"UK\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Bill\", \"design\", \"UK\"}, \"Apple\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna: {Lenovo=16.666666666666664, Apple=16.666666666666664}\n",
      "Bill: {Lenovo=16.666666666666664, Apple=16.666666666666664}\n",
      "design: {Lenovo=5.0E-9, Apple=50.0}\n",
      "US: {Lenovo=33.33333333333333, Apple=33.33333333333333}\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Anna: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"Bill: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"US: \" + naiveBayes.classify(new String[]{\"US\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still nothing surprising. However, one of the strengths of NB is the ability to combine attributes to get insights. Let's see that adding another user, Claire. Claire will buy Lenovos for all the offices and departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(new String[]{\"Claire\", \"accounting\", \"US\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Claire\", \"design\", \"US\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Claire\", \"accounting\", \"UK\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Claire\", \"design\", \"UK\"}, \"Lenovo\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claire: {Lenovo=40.0, Apple=3.0E-9}\n",
      "design: {Lenovo=20.0, Apple=30.0}\n",
      "design US: {Lenovo=11.428571428571427, Apple=20.0}\n",
      "Bill accounting: {Lenovo=70.0, Apple=30.0}\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Claire: \" + naiveBayes.classify(new String[]{\"Claire\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"design US: \" + naiveBayes.classify(new String[]{\"design\", \"US\"}).toString());\n",
    "System.out.println(\"Bill accounting: \" + naiveBayes.classify(new String[]{\"Bill accounting\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to see the usefulness of combining attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A classifier consisting of logical rules for traversing a tree from its root to a leaf node, with attributes of the entity modeled serving as predicates for the aforementioned logical rules. The leaf nodes then correspond to the class to which the given input is predicted.\n",
    "\n",
    "Consider the slightly contrived example of wanting to classify a person as adult or child based solely on the attributes of height and weight. One possible decision tree for this problem is shown below:\n",
    "\n",
    "<img src=\"images/BasicDecisionTree.png\">\n",
    "\n",
    "Clearly, this does not account for youth growth spurts either vertical or horizontal! However, it illustrates the architecture of a decision tree. \n",
    "\n",
    "With this in mind, the question of how one builds a practical decision tree classifier comes to the surface. The criteria for separating input attributes such that a class can be assigned relies on determining what logical rules, or splits, will provide the most benefit. This benefit can be pursued in a greedy fashion, wherein the best split for a given node is determined, or a long game fashion wherein an exhaustive tree is created, then pruned back to a more manageable and generalized form. \n",
    "\n",
    "The notion of a best split relies upon a cost function which provides a metric for assessment. For decision trees, this cost function needs to consider how many classes are included in the training data for which the split is being considered. One means for quantifying this inclusion is to use the so-called Gini Index, which provides a notion of purity of input training data to a leaf node. In this case, purity implies a smaller number of distinct classes, giving a Gini Index value of 0 when only one class is represented, 0.5 when two classes are evenly represented, and so on. The Gini Index is defined as:\n",
    "\n",
    "$$ \n",
    "G = \\sum(p_{k} * (1 â€“ p_{k})) \n",
    "$$\n",
    "\n",
    "\n",
    "where $G$ is the Gini index over all classes, and $p_{k}$ are the proportion of training instances with class $k$. \n",
    "\n",
    "For the generalized node case (i.e. beyond leaf nodes) the Gini score is further expanded as follows:\n",
    "\n",
    "$$\n",
    "G = \\sum_{i}((1 - \\sum_{j}(g_{i,j}^2))*(ng_{i}/n))\n",
    "$$\n",
    "\n",
    "where $G$ is the Gini score for the node, $i$ is iterated for all groups of contributing parent nodes, $j$ is iterated for all classes, and $g_{i,j} is the proportion of instances in group $i$ for class $j$, whilst $ng_{i}/n$ is the proportion of the training set for a given group. \n",
    "\n",
    "\n",
    "Beyond the means for assessing the quality of splits, there remains a need for knowing when to stop splitting. Naturally, splitting could continue until all training vectors are mapped to a leaf node. It will be discussed below why this is disadvantageous. Other criteria can be utilized, such as a maximum tree depth or width, or a threshold on cost function. \n",
    "\n",
    "## Advantages\n",
    "**Intuitive** The architecture of the classifier should be apparent to anyone completing an intro data structures class. The resulting implementation is also accessible to anyone with basic programming knowledge: it's all ifs and elses! To wit: the height and weight attribute-driven example above could be coded as the following.\n",
    "\n",
    "```java\n",
    "if (this.getHeightInMeters() > 1.2) {\n",
    "  this.setClassification(\"Adult\");\n",
    "}\n",
    "else {\n",
    "  if(this.getWeightInKilograms() > 50) {\n",
    "    this.setClassification(\"Adult\");\n",
    "  }\n",
    "  else {\n",
    "    this.setClassification(\"Child\");\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Data flexibility** They can work with numerical and categorical data with ease; whereas other classification methods will at least require a one hot encoding of categorical variables into n binary variables where n is the number of possible values the variable can take. For example the category of department in this notebook's example would require at least two features: namely \"design\" and \"accounting\".\n",
    "\n",
    "## Disadvantages\n",
    "**Overfitting** That is, the situation arising as a result of creating a classifier which works very well for its training data but is inapplicable to input received in the wild. This can be a significant detriment to tree methods, as the training approach lends itself to human bias. \n",
    "\n",
    "**Heuristic-driven** While some techniques in machine learning can provide confidence on solution optimality, tree methods are not one of them. Intuition acquired via experience can help ameliorate this situation, however, in many cases grid search methods will be necessary to be certain of a tree's appropriateness. \n",
    "\n",
    "## Improving via ensembles\n",
    "One way to improve upon the disadvantages mentioned above should be accessible to computer scientists: namely, adding a level of indirection/expansion to the classifier architecture. By utilizing an ensemble, or collection, of trees that then contribute to a final classification, a more robust classifier is made manifest. \n",
    "\n",
    "### Random Forests\n",
    "\n",
    "In a random forest, a subset of the training data is selected, from which a decision tree is generated. TBC\n",
    "\n",
    "## Example\n",
    "\n",
    "The same minimal example as the NB case above will be used, with a Java implementation of a Decision Tree algorithm (available at https://github.com/ruivieira/java-decision-tree).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.ruivieira:decisiontree:0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.ruivieira.ml.decisiontree.DecisionTree;\n",
    "import org.ruivieira.ml.decisiontree.TreeConfig;\n",
    "import org.ruivieira.ml.decisiontree.Dataset;\n",
    "import org.ruivieira.ml.decisiontree.Item;\n",
    "import org.ruivieira.ml.decisiontree.features.StringValue;\n",
    "\n",
    "\n",
    "import java.util.Map;\n",
    "\n",
    "TreeConfig treeconfig = TreeConfig.create();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset dataset = Dataset.create();\n",
    "Item trainItem = Item.create();\n",
    "trainItem.add(\"Anna\", new StringValue(\"Apple\"));\n",
    "trainItem.add(\"design\", new StringValue(\"Apple\"));\n",
    "dataset.add(trainItem);\n",
    "dataset.size();\n",
    "treeconfig.setData(dataset);\n",
    "DecisionTree decisiontree = DecisionTree.create(treeconfig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TreeConfig{minCount=1, decision='category', entropyThreshold=0.01, maxDepth=70, data=org.ruivieira.ml.decisiontree.Dataset@48a3bb0b, ignore=[]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeconfig.toString();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringValue{data='Apple'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.valueFrequency(\"Anna\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".java",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.1+13-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
